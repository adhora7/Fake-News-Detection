{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adhora7/Fake-News-Detection/blob/main/Fake_News_Detection_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrE-SJOsDi0M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RJnzdbtD4Fv",
        "outputId": "acb9000a-8390-4d4e-fc1a-823a06e72bf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news_dataset = pd.read_csv('/content/news.csv')"
      ],
      "metadata": {
        "id": "u15QfCRpD_ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Diagnosis\n"
      ],
      "metadata": {
        "id": "_4dace71ETXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(news_dataset.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7uHH1AUENnr",
        "outputId": "6eeb141e-4d90-4af3-a272-86410664a633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6335 entries, 0 to 6334\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   Unnamed: 0  6335 non-null   int64 \n",
            " 1   title       6335 non-null   object\n",
            " 2   text        6335 non-null   object\n",
            " 3   label       6335 non-null   object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 198.1+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(news_dataset.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X01Afw2SEi86",
        "outputId": "405db3ed-f189-46fd-9ccd-aedc8c7abbd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Unnamed: 0', 'title', 'text', 'label']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(news_dataset.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7aJlMlNEpZN",
        "outputId": "c1b7b595-211b-43a0-f8bb-c6bca3e680f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                                              title  \\\n",
            "0        8476                       You Can Smell Hillary’s Fear   \n",
            "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
            "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
            "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
            "4         875   The Battle of New York: Why This Primary Matters   \n",
            "\n",
            "                                                text label  \n",
            "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
            "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
            "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
            "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
            "4  It's primary day in New York and front-runners...  REAL  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(news_dataset['label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzZkoyC8ItIK",
        "outputId": "b3c2b651-8e18-4824-db30-2970371b45e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "REAL    3171\n",
            "FAKE    3164\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_mapping = {'REAL': 0, 'FAKE': 1}\n",
        "news_dataset['label'] = news_dataset['label'].replace(label_mapping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XVc1RB-IzD7",
        "outputId": "a68d4020-97cb-49b6-da38-6bc345c9d9d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1918897413.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  news_dataset['label'] = news_dataset['label'].replace(label_mapping)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(news_dataset['label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk8EqnOkJMY1",
        "outputId": "03523d7d-a8a5-4836-908c-06467836e72a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "0    3171\n",
            "1    3164\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news_dataset = news_dataset.fillna('')"
      ],
      "metadata": {
        "id": "g_6MKUeVJTU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Checking which columns have actual content"
      ],
      "metadata": {
        "id": "O0dsTJZeJhPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "content_cols_to_check = ['title','text']"
      ],
      "metadata": {
        "id": "XmQ03m3BJYNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in content_cols_to_check:\n",
        "    if column in news_dataset.columns:\n",
        "      non_empty = (news_dataset[column].astype(str).str.strip().str.len() > 0).sum()\n",
        "\n",
        "\n",
        "      #print(f\"\\n{column}:\")\n",
        "      print(column + \":\")\n",
        "      print(f\"  Non-empty (after fillnull): {non_empty}/{len(news_dataset)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owJHdy9xJo50",
        "outputId": "17461252-169f-4611-b651-11826b9629f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "title:\n",
            "  Non-empty (after fillnull): 6335/6335\n",
            "text:\n",
            "  Non-empty (after fillnull): 6299/6335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "content_column_used = None\n",
        "if 'text' in news_dataset.columns and (news_dataset['text'].astype(str).str.strip().str.len() > 0).sum() > len(news_dataset) * 0.7: # Check if 'text' has substantial content\n",
        "    print(\"\\n Using 'text' column for content\")\n",
        "    news_dataset['content'] = news_dataset['text'].astype(str)\n",
        "    content_column_used = 'text'\n",
        "elif 'title' in news_dataset.columns and (news_dataset['title'].astype(str).str.strip().str.len() > 0).sum() > len(news_dataset) * 0.7:\n",
        "     print(\"\\n⚠️ No substantial 'text' column found, using 'title'\")\n",
        "     news_dataset['content'] = news_dataset['title'].astype(str)\n",
        "     content_column_used = 'title'\n",
        "elif 'subject' in news_dataset.columns and (news_dataset['subject'].astype(str).str.strip().str.len() > 0).sum() > len(news_dataset) * 0.7:\n",
        "     print(\"\\n⚠️ No substantial 'text' or 'title' column found, using 'subject'\")\n",
        "     news_dataset['content'] = news_dataset['subject'].astype(str)\n",
        "     content_column_used = 'subject'\n",
        "\n",
        "\n",
        "news_dataset['content'] = news_dataset['content'].astype(str).fillna('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctxbZKTdg3ts",
        "outputId": "a46360b5-16f6-4094-8ab4-f1cb4a35c276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Using 'text' column for content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(content):\n",
        "    if not isinstance(content, str) or len(content) < 10:\n",
        "        return \"\"\n",
        "\n",
        "    # Remove URLs\n",
        "    content = re.sub(r'http\\S+', '', content)\n",
        "    # Remove email addresses\n",
        "    content = re.sub(r'\\S+@\\S+', '', content)\n",
        "    # Remove special characters, keeping spaces\n",
        "    content = re.sub('[^a-zA-Z\\s]', ' ', content)\n",
        "    content = content.lower()\n",
        "    content = content.split()\n",
        "\n",
        "    # Remove stopwords and short words\n",
        "    content = [wordnet_lemmatizer.lemmatize(word) for word in content\n",
        "               if word not in stopwords.words('english') and len(word) > 2]\n",
        "\n",
        "    return ' '.join(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzLDpnQAiWOi",
        "outputId": "0e7bbb83-03ab-4b89-c91a-5d58e98c74e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:12: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:12: SyntaxWarning: invalid escape sequence '\\s'\n",
            "/tmp/ipython-input-3139763831.py:12: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  content = re.sub('[^a-zA-Z\\s]', ' ', content)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nPreprocessing text from '{content_column_used}' column...\")\n",
        "news_dataset['content'] = news_dataset['content'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAmV3R0ejMy7",
        "outputId": "f9891ce8-e9cd-4e64-b412-042019880603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preprocessing text from 'text' column...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove documents with insufficient content after preprocessing\n",
        "min_words = 10  # Lowered minimum words slightly\n",
        "news_dataset['word_count'] = news_dataset['content'].str.split().str.len()\n",
        "print(f\"\\nWord count statistics (after preprocessing):\")\n",
        "print(news_dataset['word_count'].describe())\n",
        "\n",
        "news_dataset = news_dataset[news_dataset['word_count'] >= min_words]\n",
        "print(f\"\\nDataset shape after removing short documents: {news_dataset.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtneXadkng1c",
        "outputId": "8aeec6e4-c190-4eab-f60d-e2f1bcf90036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word count statistics (after preprocessing):\n",
            "count    6335.000000\n",
            "mean      425.133860\n",
            "std       452.511623\n",
            "min         0.000000\n",
            "25%       160.000000\n",
            "50%       333.000000\n",
            "75%       560.000000\n",
            "max      9974.000000\n",
            "Name: word_count, dtype: float64\n",
            "\n",
            "Dataset shape after removing short documents: (6187, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check content length by label\n",
        "if not news_dataset.empty:\n",
        "    print(\"\\nAverage word count by label (after filtering):\")\n",
        "    print(news_dataset.groupby('label')['word_count'].mean())\n",
        "else:\n",
        "    print(\"\\nNo data remaining after filtering by word count.\")\n",
        "\n",
        "\n",
        "# Check label distribution after filtering\n",
        "print(\"\\nLabel Distribution (after filtering short docs):\")\n",
        "print(news_dataset['label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hcQf43N323x",
        "outputId": "5a4d47dc-bae4-4a50-fcf3-0799d484b6c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average word count by label (after filtering):\n",
            "label\n",
            "0    494.974401\n",
            "1    375.683973\n",
            "Name: word_count, dtype: float64\n",
            "\n",
            "Label Distribution (after filtering short docs):\n",
            "label\n",
            "1    3101\n",
            "0    3086\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Check if labels are meaningful (re-check after filtering)\n",
        "\n",
        "print(\"CHECKING LABEL QUALITY (After Filtering):\")\n",
        "\n",
        "\n",
        "if not news_dataset.empty:\n",
        "\n",
        "    print(\"\\nSample REAL news (label=0):\")\n",
        "    real_samples = news_dataset[news_dataset['label'] == 0]['content'].head(3)\n",
        "    for i, text in enumerate(real_samples, 1):\n",
        "        print(f\"\\nReal {i}: {text[:200]}...\") # Print first 200 chars\n",
        "\n",
        "    print(\"\\n\\nSample FAKE news (label=1):\")\n",
        "    fake_samples = news_dataset[news_dataset['label'] == 1]['content'].head(3)\n",
        "    for i, text in enumerate(fake_samples, 1):\n",
        "        print(f\"\\nFake {i}: {text[:200]}...\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQTLgd5N7gmA",
        "outputId": "16236dcc-0932-47eb-9798-65353ea42812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHECKING LABEL QUALITY (After Filtering):\n",
            "\n",
            "Sample REAL news (label=0):\n",
            "\n",
            "Real 1: secretary state john kerry said monday stop paris later week amid criticism top american official attended sunday unity march terrorism kerry said expects arrive paris thursday evening head home week ...\n",
            "\n",
            "Real 2: primary day new york front runner hillary clinton donald trump leading poll trump vowing win enough delegate clinch republican nomination prevent contested convention sen ted cruz texas bernie sander ...\n",
            "\n",
            "Real 3: czech stockbroker saved jewish child nazi germany died age dubbed britain schindler nicholas winton arranged transport jewish youngster prague germany annexed czechoslovakia march though child origina...\n",
            "\n",
            "\n",
            "Sample FAKE news (label=1):\n",
            "\n",
            "Fake 1: daniel greenfield shillman journalism fellow freedom center new york writer focusing radical islam final stretch election hillary rodham clinton gone war fbi word unprecedented thrown around often ele...\n",
            "\n",
            "Fake 2: google pinterest digg linkedin reddit stumbleupon print delicious pocket tumblr two fundamental truth world paul ryan desperately want president paul ryan never president today proved particularly sta...\n",
            "\n",
            "Fake 3: kaydee king november lesson tonight dem loss time democrat start listening voter stop running establishment candidate people bernie november dems want tight race worked bernie walker bragman november ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if news_dataset.shape[0] > 0:\n",
        "\n",
        "    print(\"TRAINING MODELS\")\n",
        "\n",
        "\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        ngram_range=(1, 2),           # Use 1-2 grams\n",
        "        max_features=7000,            # Limit features to a reasonable number\n",
        "        min_df=5,                      # Must appear in at least 5 docs\n",
        "        max_df=0.8,                   # Ignore terms in >80% of docs\n",
        "        sublinear_tf=True,\n",
        "        use_idf=True\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        X = vectorizer.fit_transform(news_dataset['content'])\n",
        "        Y = news_dataset['label'].values\n",
        "        print(f\"\\nFeature matrix shape: {X.shape}\")\n",
        "        print(f\"Label distribution:\\n{pd.Series(Y).value_counts()}\")\n",
        "\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        print(f\"\\nSample features: {list(feature_names[:20])}\")\n",
        "\n",
        "    except Exception as e:\n",
        "         print(f\"\\nError during TF-IDF vectorization: {e}\")\n",
        "         X = None # Indicate failure\n",
        "         Y = None # Indicate failure\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zv7lH1kY8bN4",
        "outputId": "3bb65d91-7b9f-4cf3-ea31-c6839df590ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING MODELS\n",
            "\n",
            "Feature matrix shape: (6187, 7000)\n",
            "Label distribution:\n",
            "1    3101\n",
            "0    3086\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sample features: ['abandon', 'abandoned', 'abc', 'abc news', 'abdullah', 'abedin', 'ability', 'able', 'aboard', 'abortion', 'abortion right', 'abraham', 'abroad', 'absence', 'absent', 'absentee', 'absolute', 'absolutely', 'absurd', 'abu']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Proceed with training only if vectorization was successful\n",
        "if X is not None and Y is not None and X.shape[0] > 0:\n",
        "    # ===== Split data =====\n",
        "    # Ensure enough samples are available for splitting\n",
        "    if X.shape[0] >= 2 and (pd.Series(Y).value_counts() >= 2).all(): # Need at least 2 samples total and at least 2 per class for stratify\n",
        "        X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "            X, Y,\n",
        "            test_size=0.2,\n",
        "            stratify=Y,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        print(f\"\\nTraining samples after split: {X_train.shape[0]}\")\n",
        "        print(f\"Test samples after split: {X_test.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cc7UP9vOcqy6",
        "outputId": "ce275593-c3b6-4872-c264-a4c9e2a9bbb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training samples after split: 4949\n",
            "Test samples after split: 1238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Y_train and Y_test to flattened integer numpy arrays for scikit-learn compatibility\n",
        "Y_train_flat = Y_train.astype(int).ravel()\n",
        "Y_test_flat = Y_test.astype(int).ravel()"
      ],
      "metadata": {
        "id": "JyQmtDSK5puG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature importance\n",
        "if hasattr(lr_model, 'coef_') and len(feature_names) > 0:\n",
        "    feature_importance = np.abs(lr_model.coef_[0])\n",
        "    # Ensure we don't ask for more features than exist\n",
        "    num_features_to_show = min(20, len(feature_names))\n",
        "    top_indices = np.argsort(feature_importance)[-num_features_to_show:]\n",
        "\n",
        "    print(f\"\\nTop {num_features_to_show} most important words for classification:\")\n",
        "    # Sort in descending order of importance\n",
        "    for idx in reversed(top_indices):\n",
        "        print(f\"- {feature_names[idx]}: {feature_importance[idx]:.4f}\")\n",
        "else:\n",
        "    print(\"\\nCould not determine feature importance (model fitting may have failed or no features).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwOCQ33MfVCV",
        "outputId": "39fcd2f4-03e4-4395-dba3-c0911c7d9a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20 most important words for classification:\n",
            "- october: 4.5931\n",
            "- said: 3.5286\n",
            "- november: 3.0800\n",
            "- article: 2.9133\n",
            "- republican: 2.9121\n",
            "- hillary: 2.8543\n",
            "- election: 2.8442\n",
            "- share: 2.7174\n",
            "- conservative: 2.6942\n",
            "- president: 2.3988\n",
            "- source: 2.1653\n",
            "- obama: 2.1272\n",
            "- cruz: 2.0992\n",
            "- email: 2.0659\n",
            "- sen: 2.0422\n",
            "- debate: 2.0290\n",
            "- say: 2.0154\n",
            "- via: 2.0147\n",
            "- attack: 1.9658\n",
            "- russia: 1.9442\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "Y_pred = lr_model.predict(X_test)\n",
        "\n",
        "print(\"\\nModel Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy_score(Y_test_flat, Y_pred):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(Y_test_flat, Y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThSikTj0kHsB",
        "outputId": "0f23d098-c98e-4d97-bf45-94d146d8412b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Evaluation:\n",
            "Accuracy: 0.9402\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.93      0.94       617\n",
            "           1       0.93      0.95      0.94       621\n",
            "\n",
            "    accuracy                           0.94      1238\n",
            "   macro avg       0.94      0.94      0.94      1238\n",
            "weighted avg       0.94      0.94      0.94      1238\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "LFx2v80st4no",
        "outputId": "9b830969-363a-49d8-f80b-ac31a23da1a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 9)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get news text from user input\n",
        "user_news = input(\"Enter the news text to check: \")\n",
        "\n",
        "processed_user_news = preprocess_text(user_news)\n",
        "\n",
        "if 'vectorizer' in locals() and vectorizer is not None:\n",
        "    user_news_vector = vectorizer.transform([processed_user_news])\n",
        "\n",
        "    if 'lr_model' in locals() and lr_model is not None:\n",
        "        prediction = lr_model.predict(user_news_vector)\n",
        "\n",
        "        # Output the prediction\n",
        "        predicted_label = \"REAL\" if prediction[0] == 0 else \"FAKE\"\n",
        "        print(f\"The news article is predicted to be: {predicted_label}\")\n",
        "    else:\n",
        "        print(\"Error: Model not trained. Please run the training cells.\")\n",
        "else:\n",
        "    print(\"Error: TF-IDF vectorizer not fitted. Please run the vectorization cell.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHh6hGjmumba",
        "outputId": "551f3cb5-f50c-4198-f2be-ba337921c060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the news text to check: breaking news,bd's ex pm hasina died\n",
            "The news article is predicted to be: FAKE\n"
          ]
        }
      ]
    }
  ]
}